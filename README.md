# BlueBikesML

# Rider Demand Prediction (Machine Learning Project)

## Executive Summary
  Bluebikes experiences highly variable rider demand based on seasonality, day of week, and time of day, which directly affects staffing, bicycle distribution, inventory, and fleet maintenance planning. This project developed a machine learning forecasting model to accurately predict total rider count and support operational decision-making. After cleaning and engineering temporal features from the raw dataset, multiple models were developed and evaluated, including Linear Regression, Regression Tree, Random Forest (Bagging), and a Stacking Ensemble.

  This means the model reduces forecasting mistakes by more than 92% compared to not using a model at all, giving us a much more accurate and reliable understanding of how many riders to expect at any given time. We also reduced the average size of our prediction mistakes (RMSE) from about 182 riders off to about 68 riders off — a 62% improvement. Instead of being nearly 200 bikes wrong on a typical forecast, we are now usually within about 70, enabling far more precise operational planning. These improvements support better balancing of bike availability across stations, optimization of rebalancing logistics, reduced operational costs, and improved customer experience by minimizing stockouts and excess inventory. This model can be used to support daily demand forecasting, seasonal planning, and staffing allocation.


## R Libraries and Utilities

- `lubridate` — date/time parsing and feature extraction
- `rpart` — regression tree modeling
- `rpart.plot` — tree visualization
- `randomForest` — bagging ensemble model
- `BabsonAnalytics.R` — helper script for `easyPrune()` tree pruning
- `stats` — built-in linear regression modeling

## Project Walk-through:
**Preliminary**
**Step 1. Load & Clean Data**
```r
# Load data
df <- read.csv("data/train.csv", stringsAsFactors = FALSE)

# Source helper script (for pruning later)
source("scripts/BabsonAnalytics.R")

# Load required libraries
library(lubridate)
library(rpart)
library(rpart.plot)
library(randomForest)

# Convert field types / remove leakage
df$season     <- as.factor(df$season)
df$holiday    <- as.logical(df$holiday)
df$workingday <- as.logical(df$workingday)
df$weather    <- as.factor(df$weather)

df$registered <- NULL     # avoid target leakage
df$casual     <- NULL     # avoid target leakage

# Datetime parsing and feature engineering
df$datetime <- ymd_hms(df$datetime)
df$hour     <- hour(df$datetime)
df$day      <- wday(df$datetime)
df$month    <- month(df$datetime, label = TRUE)

# Convert engineered time columns to categorical
df[, c("hour","day","month")] <- lapply(df[, c("hour","day","month")], factor)

# Remove original datetime field
df$datetime <- NULL

# Preview structure
str(df)
names(df)
```
**Step 2. Partition Data**
```r
set.seed(1234)
training_cases <- sample(nrow(df), round(nrow(df) * 0.60))

train <- df[training_cases, ]
test  <- df[-training_cases, ]
```
**Step 3. Build Baseline Linear Regression Model**
```r
model_lr <- lm(count ~ ., data = train)
model_lr <- step(model_lr)      # Stepwise selection
summary(model_lr)

predictions_lr <- predict(model_lr, test)
observations   <- test$count

errors_lr <- observations - predictions_lr
rmse_lr   <- sqrt(mean(errors_lr^2))
mape_lr   <- mean(abs(errors_lr / observations))
```
### Linear Regression Model Performance Visualization
<img width="1324" height="802" alt="BlueBikesLR" src="https://github.com/user-attachments/assets/1c8056bb-882d-4416-b441-e762924369f0" />

### Findings — Linear Regression Model

The Observed vs Predicted scatter plot above compares actual rider demand with the predictions generated by the linear regression model. The **red dashed line** represents perfect prediction accuracy, where observed and predicted values would be equal. The **blue trend line** represents the model’s best fit based on the data.

The results show that while the linear regression model captures the general upward trend in rider demand, it struggles to accurately model the full range of variation. Specifically, the model tends to **underpredict high-demand periods** (e.g., commuter rush hours, weekends, and peak summer months) and occasionally **overpredict low-usage periods**. This pattern indicates that the linear model cannot fully capture the nonlinear relationships driven by time-based demand patterns and other external factors.

These findings highlight the limitations of using a simple linear regression model for forecasting rider demand and support the need for more advanced models. This motivated the progression to **Regression Trees, Random Forest (Bagging), and ultimately a Stacking Ensemble**, which substantially improved predictive accuracy and reduced error.

**Step 4. Regression Tree**
```r
model_rt <- rpart(count ~ ., data = train)
predictions_rt <- predict(model_rt, test)
errors_rt <- observations - predictions_rt
rmse_rt   <- sqrt(mean(errors_rt^2))
mape_rt   <- mean(abs(errors_rt / observations))

# Grow large tree and prune
stopping_rules <- rpart.control(minbucket = 200, minsplit = 100, cp = 0)
model_rt_big   <- rpart(count ~ ., data = train, control = stopping_rules)

model_prune_rt <- easyPrune(model_rt_big)
rpart.plot(model_prune_rt)

prediction_prune_rt <- predict(model_prune_rt, test)
error_prune_rt <- observations - prediction_prune_rt
rmse_prune_rt  <- sqrt(mean(error_prune_rt^2))
mape_prune_rt  <- mean(abs(error_prune_rt / observations))
```
### Regression Tree Visualization
<img width="1324" height="802" alt="BlueBikesRT" src="https://github.com/user-attachments/assets/54e41f4a-9233-42ae-ac5c-18dda702c251" />

### Findings — Regression Tree Model

The regression tree visualization above illustrates how the model uses temporal and weather-related variables to predict rider demand. The first split occurs on **hour of day**, indicating that time-of-day is the strongest driver of riding behavior. This aligns with real-world usage trends such as commuter peaks and reduced nighttime usage.

Subsequent splits are driven by **temperature**, **month of the year**, **day of the week**, and **working day status**, showing that rider demand is highly sensitive to seasonal and environmental factors. For example, the model identifies significantly higher demand during warmer temperatures and typical commuting hours (7 AM–9 AM and 10 AM–9 PM), while very low demand occurs during early morning hours and colder periods (e.g., winter months Jan–Apr with temperatures below ~10°C).

This structure demonstrates how tree-based models capture **nonlinear relationships** that a simple linear regression cannot represent. While the regression tree improves interpretability and accuracy, its depth still leaves room for instability and overfitting—supporting the transition to **Random Forest and Stacking Ensemble models**, which combine many trees to produce more stable and accurate forecasts.

**Step 5. Bagging / Random Forest**
```r
rf <- randomForest(count ~ ., data = train, ntree = 500)

pred_rf <- predict(rf, test)
errors_rf <- observations - pred_rf

rmse_rf  <- sqrt(mean(errors_rf^2))
mape_rf  <- mean(abs(errors_rf / observations))
```
### Model Performance Metrics (RMSE & MAPE)

| Model             | RMSE   | MAPE  |
|-------------------|--------|-------|
| Baseline          | 181.54 | 7.93% |
| Linear Regression | 110.66 | 2.48% |
| Regression Tree   | 98.31  | 1.00% |
| Random Forest     | 72.79  | 1.09% |

### Random Forest Observed vs Predicted Visualization
<img width="1324" height="802" alt="BlueBikesObservedVsPredictedRF" src="https://github.com/user-attachments/assets/8b042d04-16d1-4cf0-aeec-7c752c4bcbf8" />
The plot above compares the Random Forest model’s predicted rider counts with the actual observed values. The **red diagonal line** represents perfect prediction accuracy, while the **blue trend line** reflects the model’s actual fitted trend. Unlike the Linear Regression plot earlier, the Random Forest plot shows a much tighter clustering of points around the identity line.

This improvement visually confirms the significant reduction in prediction error achieved by Random Forest, especially during high-demand periods where the linear model struggled most.

### Feature Importance - Random Forest
<img width="1324" height="802" alt="BlueBikesRF" src="https://github.com/user-attachments/assets/7aca2589-ef69-4d84-963f-4c321b2e2abd" />

### Findings — Random Forest Model

The Random Forest model significantly improves predictive accuracy compared to both the Linear Regression and Regression Tree models. By averaging predictions across hundreds of trees, the model reduces variance and captures complex nonlinear interactions that simpler models cannot.

The variable importance analysis shows that **hour of day** is by far the strongest predictor of rider demand, followed by **apparent temperature (atemp)**, **temperature**, and **humidity**. These patterns reinforce real-world usage behavior: demand grows during typical commuting hours and increases substantially in warmer and more comfortable weather conditions. In contrast, variables such as **holiday**, **weather category**, and **working day status** play relatively minor roles in prediction.

The Random Forest reduces RMSE to **72.79** (from 110.66 for Linear Regression and 98.31 for the Regression Tree), meaning predictions are on average about 73 riders off from actual demand—representing a substantial improvement relative to the baseline RMSE of 181.54. This increased accuracy makes Random Forest a strong operational forecasting tool.

**Step 6. Stacking Ensemble Model**
```r
# Predictions of all models for stacked learning
pred_lr_full <- predict(model_lr, df)
pred_rt_full <- predict(model_rt, df)
pred_rf_full <- predict(rf, df)

df_stacked <- cbind(df, pred_lr_full, pred_rt_full, pred_rf_full)

training_stacked <- df_stacked[training_cases, ]
test_stacked     <- df_stacked[-training_cases, ]

stacked <- randomForest(count ~ ., data = training_stacked, ntree = 500)
pred_stacked <- predict(stacked, test_stacked)

errors_stacked <- observations - pred_stacked
rmse_stacked   <- sqrt(mean(errors_stacked^2))
mape_stacked   <- mean(abs(errors_stacked / observations))
```
### Final Model Comparison — Original vs. Stacking Ensemble

| Model                | RMSE (Original) | MAPE (Original) | RMSE (Stacking) | MAPE (Stacking) |
|----------------------|-----------------|-----------------|-----------------|-----------------|
| Baseline             | 181.54          | 7.93%           | —               | —               |
| Linear Regression    | 110.66          | 2.48%           | 68.26           | 0.62%           |
| Regression Tree      | 98.31           | 1.00%           | 68.26           | 0.62%           |
| Random Forest        | 72.79           | 1.09%           | 68.26           | 0.62%           |
| **Stacking Ensemble**| **68.26**       | **0.62%**       | **68.26**       | **0.62%**       |

### Findings — Stacking Ensemble Model

The Stacking Ensemble model delivers the strongest overall predictive performance by combining the strengths of multiple models (Linear Regression, Regression Tree, and Random Forest). Instead of relying on a single learning method, stacking learns how to optimally weight the predictions from each base model, producing a more stable and accurate final prediction.

The Stacking model reduces RMSE to **68.26**, improving upon the already strong Random Forest RMSE of **72.79** and dramatically outperforming both the baseline (**181.54**) and Linear Regression (**110.66**). With a MAPE of **0.62%**, the model predicts rider demand within less than **1% of the true number of riders** on average, making it highly reliable for operational forecasting.

These improvements demonstrate the value of ensemble methods in capturing complex seasonal, hourly, and weather-driven patterns that cannot be accurately represented by simpler individual models.

### Final Recommendation & Business Impact

Based on the comparative model evaluation, the **Stacking Ensemble model is recommended as the final forecasting solution** for Bluebikes rider demand prediction. The model provides industry-leading accuracy, lowering forecasting error by more than **92%** compared to the baseline method of using historical averages and reducing RMSE from approximately **182 riders to 68 riders**—a **62% improvement** in average prediction accuracy.

With this level of precision, Bluebikes can make more informed decisions across multiple areas of daily operations, including:

- **Bike distribution & rebalancing** — ensuring bicycles are available where riders need them
- **Staffing & fleet scheduling** — aligning labor with predicted demand and reducing overtime inefficiencies
- **Maintenance & inventory planning** — optimizing resource allocation by anticipating seasonal ridership patterns
- **Customer experience improvement** — reducing stockouts and overcrowded stations during peak demand

The Stacking Ensemble model can be integrated into daily forecasting workflows or deployed as part of an automated dashboard to support real-time decision-making. Future enhancements may include geographic station-level modeling, deep learning approaches, or automated hyperparameter tuning for additional performance gains.
